{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecd56b43",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Scala\n",
    "\n",
    "We found that the approach that we took during this project was generally unsuccessful. With not enough iterations and low conductivity in edges it meant that we were not able to provide an accurate replication of social sentiment through an office in either approach. While we found that approach 1 provided more expected and useful results, it was not good enough to provide results that we felt comfortable drawing conclusions on. \n",
    "\n",
    "Therefore, while we were unsuccessful in finding employees that are likely to be insider threats, we have a firm basis for futher improvements that could lead to some very useful results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369fd25",
   "metadata": {},
   "source": [
    "## Time Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b4a334-cd66-4859-bd1f-662377dc38ee",
   "metadata": {},
   "source": [
    "We ran connectivity checks and the PageRank algorithm on Erdos-Renyi graphs with varying nodes and probabilities on both PySpark-GraphFrames and R-igraph. Initially, we expected PySpark-GraphFrames to scale better than R-igraph, which is indeed what our evidence points to. \n",
    "\n",
    "Connectivity checks were overall slower on PySpark-GraphFrames for all nodes and probability combinations, but the increase in runtime magnitude was smaller than that for R-igraph (a slowdown of around 4.4x compared to 19x). This implies that PySpark-GraphFrames is slower up to a constant for small number of nodes/edges but may be much faster as this number becomes increasingly large. We did not have the time or compute to check increasing orders of node numbers, however, so we did not find any instances where PySpark-GraphFrames was directly faster than R-igraph.\n",
    "\n",
    "PageRank ran at an almost constant runtime for all node/probability combinations for PySpark-GraphFrames, while it slowed down significantly for R-igraph. It was significantly faster than connectivity checks on PySpark-GraphFrames, but the two algorithms performed at roughly the same speed on R-igraph. Initially, PySpark-GraphFrames was rouhgly 10x slower to start, but ended up being 2.5x faster on our highest number of nodes (10,000). We concluded that this had something to do with the connectivity check having to call on Spark dataframes, which appears to be the main time bottleneck (though potentially only up to a constant), whereas PageRank does not do this and so runtime was fairly constant.\n",
    "\n",
    "Overall, PySpark-GraphFrames seems to scale better than R-igraph and it indeed makes more sense to use for large datasets, not just because of the better scalability but also because big datasets require large amounts of storage space and RAM to deal with, and PySpark works with RDDs and can be distributed over many devices, whereas R-igraph would require it all to run on a single machine which is cost-expensive in comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f369323d",
   "metadata": {},
   "source": [
    "## Improvements\n",
    "\n",
    "- As we have mentioned a few times, it would likely provide better results if we were to have more than 2 iterations of sentiment spread.\n",
    "- We could have normalised the weightings during the program rather than during the analysis, this would ensure that the results are more reasonable and more understanable, without impacting the structure of the data.\n",
    "- It is possible that due to the truncation for values at various points during the Scala implementation, it may have truncated the scientific notation e.g instead of 9.0e-4 it just uses 9. This is a possible reason behind the unexpected values produced by approach 2.\n",
    "- We could have used a non-linear method for accounting for when multiple emails have the same sender and reciever. For example a gaussian could account for a 'sweet spot' more emails is better but not too many, this would be useful for networks with newsletters (how many people actaully read them after all). Or likely a better method would be to use an exponential function, providing a scenarios where more is always better. \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
